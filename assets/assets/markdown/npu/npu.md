# Single flux quantum for neural processing units (NPU)

_July 15, 2024_

As we reach the physical limits of Moore’s law with CMOS, we have to explore newer technologies to keep advancing computing power. Single-Flux Quantum (SFQ) technology is one of the approaches, which achieves picosecond speeds and low energies as low as 1eV per logic operation. A group of researchers have worked on developing a Single-Flux Quantum (SFQ) logic-based Neural Processing Unit (NPU)- SuperNPU. This is based on their article published in IEEE-Micro.

The research identifies important characteristics of SFQ architecture: simple control flow, sequential design like shift register memory for on-chip memory, and CMOS memory for off-chip memory. Considering these architectural requirements, the researchers developed an SFQ-based NPU unit. Its on-chip buffer is based on shift register-based memory. The network unit uses the systolic array design since it can reach very high clock frequency and takes less area than the splitter tree design. The processing element (PE) utilizes weight-stationary (WS) data flow because it can maximize frequency than the output-stationary (OS) data flow which includes feedback loops. This high frequency is achieved by the point-to-point synchronization in SFQ by propagating data and clock pulse in the same direction.

To test and improve SFQ architecture, the researchers designed and validated a simulation framework consisting of two main engines: SFQ-NPU estimator and SFQ-NPU simulator. The estimator conducts analysis on device/gate level, micro-architecture level, and architecture level. At gate level, timing, power, and area parameters are calculated for rapid-SFQ (RSFQ) or energy-efficient-RSFQ (ERSFQ), and at micro-architecture level, frequency, static power, access energy, and area parameters are evaluated. Both microarchitecture level estimations and inter-unit connections are considered for architecture level estimations of the NPU. Based on frequency and power estimations from the estimator, the simulator decides on the overall performance and power for deep neural network (DNN) applications.

A performance analysis of the simulation framework was conducted for different CNNs (Alexnet, fastRCNN, GoogLe-Net, MobileNet, ResNet50, VGG16), using AIST1.0 micrometer process, 300GBps memory bandwidth, and area less than 30mm² (values comparable to TPU core). The results revealed two problems with the framework: performance is highly decreased but the data overhead among different (or single) on-chip buffers, and since off-chip CMOS memory access is much lower than the fast SFQ computing units, overall PE is underutilized. So, this baseline design with 52.6GHz clock frequency cannot perform better than the current CMOS designs.

To overcome these challenges, an optimal architecture SuperNPU is designed with buffer optimization, resource balancing, and an increased number of PE registers. A single buffer is now divided into many and connected via a mux and a demux, and this completely removed the buffer overhead resulting in a 19 times improvement in the performance. By increasing the on-chip buffer capacity and reducing the PE count, the frequency of slow off-chip memory access was reduced matching computation and memory speeds. The number of local weight registers inside a PE is increased to improve PE utilization This way, SuperNPU outperforms the baseline by 52 times at 52.6GHz and the TPU by 23 times. Power efficiency-wise, SuperNPU with ERSFQ is 490 times higher without cooling and 1.23 times higher with cooling than the TPU.

## Importance of the research outcome

The SFQ logic modeling framework developed by the researchers will accelerate the development of SFQ-based architecture designs. This will help architects focus more on the improvement of the architecture itself by giving easy access to model their designs.
SFQ-based hardware optimization for NPUs gives better insights into hardware optimization aspects and efficient implementation of those. It also compares the SFQ technology with CMOS and highlights its advantages and the huge potential it has in computing power.
This research opens many opportunities in SFQ research in (1) 3D stacked SFQ designs utilizing ultra-low power superconducting logic devices and (2) large-scale multichip architectures with the use of low latency lossless superconductor transmission lines.
Overall the research paves the path to move from CMOS to SFQ technology to achieve computing power requirements of the generation, which demands are increasing with the emerging AI technologies and machine learning computing. As the semiconductor industry evolves to match the requirements and as we integrate these superconductor-based logic devices, having foundations in the architectural designs is super important, and this paper contributes very much to the progress. Also, we can look forward to more research in this area and make use of the SFQ technology to meet advanced computing goals.

Refer to original paper at [https://ieeexplore.ieee.org/document/9395193](https://ieeexplore.ieee.org/document/9395193), authored by Koki Ishida, Ilkwon Byun, Ikki Nagaoka, Kosuke Fukumitsu, Masamitsu Tanaka, Satoshi Kawakami, Jangwoo Kim, Koji Inoue, published on April 5, 2021 on IEEE Micro ( Volume: 41, Issue: 3, 01 May-June 2021. DOI: 10.1109/MM.2021.3070488 )